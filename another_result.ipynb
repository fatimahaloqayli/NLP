{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc1</th>\n",
       "      <th>feq_doc1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>273</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc1  feq_doc1\n",
       "0  the       273"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "filename = 'documents/document_1.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "#print (text)\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# print (tokens)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# # remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "w = list(filter(None,stripped))\n",
    "#print(w)\n",
    "\n",
    "import operator\n",
    "freq_dis={}\n",
    "for tok in w:\n",
    "    if tok in freq_dis:\n",
    "        freq_dis[tok] += 1\n",
    "    else:\n",
    "        freq_dis[tok] = 1\n",
    "        \n",
    "sorted_freq_dist= sorted(freq_dis.items(), key=operator.itemgetter(1),\n",
    "reverse=True)\n",
    "\n",
    "doc1=pd.DataFrame(sorted_freq_dist)\n",
    "doc1.rename(columns={doc1.columns[0]: \"doc1\", doc1.columns[1]: \"feq_doc1\"}, inplace=True)\n",
    "doc1.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc2</th>\n",
       "      <th>feq_doc2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc2  feq_doc2\n",
       "0  the       322"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  load data\n",
    "filename = 'documents/document_2.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "#print (text)\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# print (tokens)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# # remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "\n",
    "f = list(filter(None,stripped))\n",
    "#print(w)\n",
    "\n",
    "import operator\n",
    "freq_dis={}\n",
    "for tok in f:\n",
    "    if tok in freq_dis:\n",
    "        freq_dis[tok] += 1\n",
    "    else:\n",
    "        freq_dis[tok] = 1\n",
    "        \n",
    "sorted_freq_dist= sorted(freq_dis.items(), key=operator.itemgetter(1),\n",
    "reverse=True)\n",
    "\n",
    "doc2 =pd.DataFrame(sorted_freq_dist)\n",
    "doc2.rename(columns={doc2.columns[0]: \"doc2\", doc2.columns[1]: \"feq_doc2\"}, inplace=True)\n",
    "doc2.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc3</th>\n",
       "      <th>feq_doc3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc3  feq_doc3\n",
       "0  the        25"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  load data\n",
    "filename = 'documents/document_3.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "#print (text)\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# print (tokens)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# # remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "\n",
    "l = list(filter(None,stripped))\n",
    "#print(w)\n",
    "\n",
    "import operator\n",
    "freq_dis={}\n",
    "for tok in l:\n",
    "    if tok in freq_dis:\n",
    "        freq_dis[tok] += 1\n",
    "    else:\n",
    "        freq_dis[tok] = 1\n",
    "        \n",
    "sorted_freq_dist= sorted(freq_dis.items(), key=operator.itemgetter(1),\n",
    "reverse=True)\n",
    "\n",
    "doc3=pd.DataFrame(sorted_freq_dist)\n",
    "doc3.rename(columns={doc3.columns[0]: \"doc3\", doc3.columns[1]: \"feq_doc3\"}, inplace=True)\n",
    "doc3.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc4</th>\n",
       "      <th>feq_doc4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc4  feq_doc4\n",
       "0  the       360"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "filename = 'documents/document_4.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "#print (text)\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# print (tokens)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# # remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "\n",
    "m = list(filter(None,stripped))\n",
    "#print(w)\n",
    "\n",
    "import operator\n",
    "freq_dis={}\n",
    "for tok in m:\n",
    "    if tok in freq_dis:\n",
    "        freq_dis[tok] += 1\n",
    "    else:\n",
    "        freq_dis[tok] = 1\n",
    "        \n",
    "sorted_freq_dist= sorted(freq_dis.items(), key=operator.itemgetter(1),\n",
    "reverse=True)\n",
    "doc4=pd.DataFrame(sorted_freq_dist)\n",
    "doc4.rename(columns={doc4.columns[0]: \"doc4\", doc4.columns[1]: \"feq_doc4\"}, inplace=True)\n",
    "doc4.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc5</th>\n",
       "      <th>feq_doc5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>612</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc5  feq_doc5\n",
       "0  the       612"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "filename = 'documents/document_5.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "#print (text)\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# print (tokens)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# # remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "\n",
    "n = list(filter(None,stripped))\n",
    "#print(w)\n",
    "\n",
    "import operator\n",
    "freq_dis={}\n",
    "for tok in n:\n",
    "    if tok in freq_dis:\n",
    "        freq_dis[tok] += 1\n",
    "    else:\n",
    "        freq_dis[tok] = 1\n",
    "\n",
    "sorted_freq_dist= sorted(freq_dis.items(), key=operator.itemgetter(1),\n",
    "reverse=True)\n",
    "\n",
    "doc5=pd.DataFrame(sorted_freq_dist)\n",
    "doc5.rename(columns={doc5.columns[0]: \"doc5\", doc5.columns[1]: \"feq_doc5\"}, inplace=True)\n",
    "doc5.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc6</th>\n",
       "      <th>feq_doc6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc6  feq_doc6\n",
       "0  the       407"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "filename = 'documents/document_6.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "#print (text)\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# print (tokens)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# # remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "\n",
    "o = list(filter(None,stripped))\n",
    "#print(w)\n",
    "\n",
    "import operator\n",
    "freq_dis={}\n",
    "for tok in o:\n",
    "    if tok in freq_dis:\n",
    "        freq_dis[tok] += 1\n",
    "    else:\n",
    "        freq_dis[tok] = 1\n",
    "\n",
    "sorted_freq_dist= sorted(freq_dis.items(), key=operator.itemgetter(1),\n",
    "reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "doc6=pd.DataFrame(sorted_freq_dist)\n",
    "doc6.rename(columns={doc6.columns[0]: \"doc6\", doc6.columns[1]: \"feq_doc6\"}, inplace=True)\n",
    "\n",
    "doc6.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc7</th>\n",
       "      <th>feq_doc7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc7  feq_doc7\n",
       "0  the       266"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "filename = 'documents/document_7.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "#print (text)\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# print (tokens)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# # remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "\n",
    "p = list(filter(None,stripped))\n",
    "#print(w)\n",
    "\n",
    "import operator\n",
    "freq_dis={}\n",
    "for tok in p:\n",
    "    if tok in freq_dis:\n",
    "        freq_dis[tok] += 1\n",
    "    else:\n",
    "        freq_dis[tok] = 1\n",
    "\n",
    "sorted_freq_dist= sorted(freq_dis.items(), key=operator.itemgetter(1),\n",
    "reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "doc7=pd.DataFrame(sorted_freq_dist)\n",
    "doc7.rename(columns={doc7.columns[0]: \"doc7\", doc7.columns[1]: \"feq_doc7\"}, inplace=True)\n",
    "\n",
    "doc7.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc8</th>\n",
       "      <th>feq_doc8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc8  feq_doc8\n",
       "0  the       504"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "filename = 'documents/document_8.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "#print (text)\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# print (tokens)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# # remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "\n",
    "q = list(filter(None,stripped))\n",
    "#print(w)\n",
    "\n",
    "import operator\n",
    "freq_dis={}\n",
    "for tok in q:\n",
    "    if tok in freq_dis:\n",
    "        freq_dis[tok] += 1\n",
    "    else:\n",
    "        freq_dis[tok] = 1\n",
    "\n",
    "sorted_freq_dist= sorted(freq_dis.items(), key=operator.itemgetter(1),\n",
    "reverse=True)\n",
    "\n",
    "\n",
    "doc8=pd.DataFrame(sorted_freq_dist)\n",
    "doc8.rename(columns={doc8.columns[0]: \"doc8\", doc8.columns[1]: \"feq_doc8\"}, inplace=True)\n",
    "\n",
    "doc8.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc9</th>\n",
       "      <th>feq_doc9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc9  feq_doc9\n",
       "0  the        23"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "filename = 'documents/document_9.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "#print (text)\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# print (tokens)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# # remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "\n",
    "r= list(filter(None,stripped))\n",
    "#print(w)\n",
    "\n",
    "import operator\n",
    "freq_dis={}\n",
    "for tok in r:\n",
    "    if tok in freq_dis:\n",
    "        freq_dis[tok] += 1\n",
    "    else:\n",
    "        freq_dis[tok] = 1\n",
    "\n",
    "sorted_freq_dist= sorted(freq_dis.items(), key=operator.itemgetter(1),\n",
    "reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "doc9=pd.DataFrame(sorted_freq_dist)\n",
    "doc9.rename(columns={doc9.columns[0]: \"doc9\", doc9.columns[1]: \"feq_doc9\"}, inplace=True)\n",
    "\n",
    "doc9.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc10</th>\n",
       "      <th>feq_doc10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc10  feq_doc10\n",
       "0   the         26"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "filename = 'documents/document_10.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "#print (text)\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# print (tokens)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# # remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "\n",
    "s= list(filter(None,stripped))\n",
    "#print(w)\n",
    "\n",
    "import operator\n",
    "freq_dis={}\n",
    "for tok in s:\n",
    "    if tok in freq_dis:\n",
    "        freq_dis[tok] += 1\n",
    "    else:\n",
    "        freq_dis[tok] = 1\n",
    "\n",
    "sorted_freq_dist= sorted(freq_dis.items(), key=operator.itemgetter(1),\n",
    "reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "doc10=pd.DataFrame(sorted_freq_dist)\n",
    "doc10.rename(columns={doc10.columns[0]: \"doc10\", doc10.columns[1]: \"feq_doc10\"}, inplace=True)\n",
    "\n",
    "doc10.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc11</th>\n",
       "      <th>feq_doc11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc11  feq_doc11\n",
       "0     1        148"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "filename = 'documents/document_11.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "#print (text)\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# print (tokens)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# # remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "\n",
    "t= list(filter(None,stripped))\n",
    "#print(w)\n",
    "\n",
    "import operator\n",
    "freq_dis={}\n",
    "for tok in t:\n",
    "    if tok in freq_dis:\n",
    "        freq_dis[tok] += 1\n",
    "    else:\n",
    "        freq_dis[tok] = 1\n",
    "\n",
    "sorted_freq_dist= sorted(freq_dis.items(), key=operator.itemgetter(1),\n",
    "reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "doc11=pd.DataFrame(sorted_freq_dist)\n",
    "doc11.rename(columns={doc11.columns[0]: \"doc11\", doc11.columns[1]: \"feq_doc11\"}, inplace=True)\n",
    "\n",
    "doc11.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc12</th>\n",
       "      <th>feq_doc12</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc12  feq_doc12\n",
       "0   the         62"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "filename = 'documents/document_12.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "#print (text)\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# print (tokens)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# # remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "\n",
    "u= list(filter(None,stripped))\n",
    "#print(w)\n",
    "\n",
    "import operator\n",
    "freq_dis={}\n",
    "for tok in u:\n",
    "    if tok in freq_dis:\n",
    "        freq_dis[tok] += 1\n",
    "    else:\n",
    "        freq_dis[tok] = 1\n",
    "\n",
    "sorted_freq_dist= sorted(freq_dis.items(), key=operator.itemgetter(1),\n",
    "reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "doc12=pd.DataFrame(sorted_freq_dist)\n",
    "doc12.rename(columns={doc12.columns[0]: \"doc12\", doc12.columns[1]: \"feq_doc12\"}, inplace=True)\n",
    "\n",
    "doc12.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc13</th>\n",
       "      <th>feq_doc13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc13  feq_doc13\n",
       "0   the         30"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "filename = 'documents/document_13.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "#print (text)\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# print (tokens)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# # remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "\n",
    "v= list(filter(None,stripped))\n",
    "#print(w)\n",
    "\n",
    "import operator\n",
    "freq_dis={}\n",
    "for tok in v:\n",
    "    if tok in freq_dis:\n",
    "        freq_dis[tok] += 1\n",
    "    else:\n",
    "        freq_dis[tok] = 1\n",
    "\n",
    "sorted_freq_dist= sorted(freq_dis.items(), key=operator.itemgetter(1),\n",
    "reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "doc13=pd.DataFrame(sorted_freq_dist)\n",
    "doc13.rename(columns={doc13.columns[0]: \"doc13\", doc13.columns[1]: \"feq_doc13\"}, inplace=True)\n",
    "\n",
    "doc13.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc14</th>\n",
       "      <th>feq_doc14</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>report</td>\n",
       "      <td>98</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    doc14  feq_doc14\n",
       "0  report         98"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "filename = 'documents/document_14.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "#print (text)\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# print (tokens)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# # remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "\n",
    "x= list(filter(None,stripped))\n",
    "#print(w)\n",
    "\n",
    "import operator\n",
    "freq_dis={}\n",
    "for tok in x:\n",
    "    if tok in freq_dis:\n",
    "        freq_dis[tok] += 1\n",
    "    else:\n",
    "        freq_dis[tok] = 1\n",
    "\n",
    "sorted_freq_dist= sorted(freq_dis.items(), key=operator.itemgetter(1),\n",
    "reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "doc14=pd.DataFrame(sorted_freq_dist)\n",
    "doc14.rename(columns={doc14.columns[0]: \"doc14\", doc14.columns[1]: \"feq_doc14\"}, inplace=True)\n",
    "\n",
    "doc14.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "################15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc15</th>\n",
       "      <th>feq_doc15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc15  feq_doc15\n",
       "0   the        261"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "filename = 'documents/document_15.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "#print (text)\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# print (tokens)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# # remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "\n",
    "y= list(filter(None,stripped))\n",
    "#print(w)\n",
    "\n",
    "import operator\n",
    "freq_dis={}\n",
    "for tok in y:\n",
    "    if tok in freq_dis:\n",
    "        freq_dis[tok] += 1\n",
    "    else:\n",
    "        freq_dis[tok] = 1\n",
    "\n",
    "sorted_freq_dist= sorted(freq_dis.items(), key=operator.itemgetter(1),\n",
    "reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "doc15=pd.DataFrame(sorted_freq_dist)\n",
    "doc15.rename(columns={doc15.columns[0]: \"doc15\", doc15.columns[1]: \"feq_doc15\"}, inplace=True)\n",
    "\n",
    "doc15.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc16</th>\n",
       "      <th>feq_doc16</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc16  feq_doc16\n",
       "0   the        106"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "filename = 'documents/document_16.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "#print (text)\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# print (tokens)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# # remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "\n",
    "r= list(filter(None,stripped))\n",
    "#print(w)\n",
    "\n",
    "import operator\n",
    "freq_dis={}\n",
    "for tok in r:\n",
    "    if tok in freq_dis:\n",
    "        freq_dis[tok] += 1\n",
    "    else:\n",
    "        freq_dis[tok] = 1\n",
    "\n",
    "sorted_freq_dist= sorted(freq_dis.items(), key=operator.itemgetter(1),\n",
    "reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "doc16=pd.DataFrame(sorted_freq_dist)\n",
    "doc16.rename(columns={doc16.columns[0]: \"doc16\", doc16.columns[1]: \"feq_doc16\"}, inplace=True)\n",
    "\n",
    "doc16.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "################17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc17</th>\n",
       "      <th>feq_doc17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>80</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc17  feq_doc17\n",
       "0   the         80"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "filename = 'documents/document_17.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "#print (text)\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# print (tokens)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# # remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "\n",
    "rr= list(filter(None,stripped))\n",
    "#print(w)\n",
    "\n",
    "import operator\n",
    "freq_dis={}\n",
    "for tok in rr:\n",
    "    if tok in freq_dis:\n",
    "        freq_dis[tok] += 1\n",
    "    else:\n",
    "        freq_dis[tok] = 1\n",
    "\n",
    "sorted_freq_dist= sorted(freq_dis.items(), key=operator.itemgetter(1),\n",
    "reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "doc17=pd.DataFrame(sorted_freq_dist)\n",
    "doc17.rename(columns={doc17.columns[0]: \"doc17\", doc17.columns[1]: \"feq_doc17\"}, inplace=True)\n",
    "\n",
    "doc17.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "################18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc18</th>\n",
       "      <th>feq_doc18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc18  feq_doc18\n",
       "0   the         18"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "filename = 'documents/document_18.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "#print (text)\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# print (tokens)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# # remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "\n",
    "aa= list(filter(None,stripped))\n",
    "#print(w)\n",
    "\n",
    "import operator\n",
    "freq_dis={}\n",
    "for tok in aa:\n",
    "    if tok in freq_dis:\n",
    "        freq_dis[tok] += 1\n",
    "    else:\n",
    "        freq_dis[tok] = 1\n",
    "\n",
    "sorted_freq_dist= sorted(freq_dis.items(), key=operator.itemgetter(1),\n",
    "reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "doc18=pd.DataFrame(sorted_freq_dist)\n",
    "doc18.rename(columns={doc18.columns[0]: \"doc18\", doc18.columns[1]: \"feq_doc18\"}, inplace=True)\n",
    "\n",
    "doc18.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "################19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc19</th>\n",
       "      <th>feq_doc19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>894</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc19  feq_doc19\n",
       "0   the        894"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "filename = 'documents/document_19.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "#print (text)\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# print (tokens)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# # remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "\n",
    "aq= list(filter(None,stripped))\n",
    "#print(w)\n",
    "\n",
    "import operator\n",
    "freq_dis={}\n",
    "for tok in aq:\n",
    "    if tok in freq_dis:\n",
    "        freq_dis[tok] += 1\n",
    "    else:\n",
    "        freq_dis[tok] = 1\n",
    "\n",
    "sorted_freq_dist= sorted(freq_dis.items(), key=operator.itemgetter(1),\n",
    "reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "doc19=pd.DataFrame(sorted_freq_dist)\n",
    "doc19.rename(columns={doc19.columns[0]: \"doc19\", doc19.columns[1]: \"feq_doc19\"}, inplace=True)\n",
    "\n",
    "doc19.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "################20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc20</th>\n",
       "      <th>feq_doc20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  doc20  feq_doc20\n",
       "0   the        963"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load data\n",
    "filename = 'documents/document_20.txt'\n",
    "file = open(filename, 'rt')\n",
    "text = file.read()\n",
    "#print (text)\n",
    "file.close()\n",
    "# split into words\n",
    "from nltk.tokenize import word_tokenize\n",
    "tokens = word_tokenize(text)\n",
    "# print (tokens)\n",
    "# convert to lower case\n",
    "tokens = [w.lower() for w in tokens]\n",
    "\n",
    "# # remove punctuation from each word\n",
    "import string\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "stripped = [w.translate(table) for w in tokens]\n",
    "\n",
    "\n",
    "ax= list(filter(None,stripped))\n",
    "#print(w)\n",
    "\n",
    "import operator\n",
    "freq_dis={}\n",
    "for tok in ax:\n",
    "    if tok in freq_dis:\n",
    "        freq_dis[tok] += 1\n",
    "    else:\n",
    "        freq_dis[tok] = 1\n",
    "\n",
    "sorted_freq_dist= sorted(freq_dis.items(), key=operator.itemgetter(1),\n",
    "reverse=True)\n",
    "\n",
    "\n",
    "\n",
    "doc20=pd.DataFrame(sorted_freq_dist)\n",
    "doc20.rename(columns={doc20.columns[0]: \"doc20\", doc20.columns[1]: \"feq_doc20\"}, inplace=True)\n",
    "\n",
    "doc20.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data frame each two columns represent the word in each document and the other for the frequency of this word in this documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc1</th>\n",
       "      <th>feq_doc1</th>\n",
       "      <th>doc2</th>\n",
       "      <th>feq_doc2</th>\n",
       "      <th>doc3</th>\n",
       "      <th>feq_doc3</th>\n",
       "      <th>doc4</th>\n",
       "      <th>feq_doc4</th>\n",
       "      <th>doc5</th>\n",
       "      <th>feq_doc5</th>\n",
       "      <th>...</th>\n",
       "      <th>doc16</th>\n",
       "      <th>feq_doc16</th>\n",
       "      <th>doc17</th>\n",
       "      <th>feq_doc17</th>\n",
       "      <th>doc18</th>\n",
       "      <th>feq_doc18</th>\n",
       "      <th>doc19</th>\n",
       "      <th>feq_doc19</th>\n",
       "      <th>doc20</th>\n",
       "      <th>feq_doc20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>273</td>\n",
       "      <td>the</td>\n",
       "      <td>322</td>\n",
       "      <td>the</td>\n",
       "      <td>25</td>\n",
       "      <td>the</td>\n",
       "      <td>360</td>\n",
       "      <td>the</td>\n",
       "      <td>612</td>\n",
       "      <td>...</td>\n",
       "      <td>the</td>\n",
       "      <td>106</td>\n",
       "      <td>the</td>\n",
       "      <td>80</td>\n",
       "      <td>the</td>\n",
       "      <td>18</td>\n",
       "      <td>the</td>\n",
       "      <td>894</td>\n",
       "      <td>the</td>\n",
       "      <td>963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>of</td>\n",
       "      <td>138</td>\n",
       "      <td>of</td>\n",
       "      <td>111</td>\n",
       "      <td>wanzer</td>\n",
       "      <td>15</td>\n",
       "      <td>dna</td>\n",
       "      <td>200</td>\n",
       "      <td>of</td>\n",
       "      <td>191</td>\n",
       "      <td>...</td>\n",
       "      <td>and</td>\n",
       "      <td>56</td>\n",
       "      <td>in</td>\n",
       "      <td>57</td>\n",
       "      <td>and</td>\n",
       "      <td>16</td>\n",
       "      <td>of</td>\n",
       "      <td>600</td>\n",
       "      <td>of</td>\n",
       "      <td>477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>and</td>\n",
       "      <td>123</td>\n",
       "      <td>and</td>\n",
       "      <td>91</td>\n",
       "      <td>a</td>\n",
       "      <td>9</td>\n",
       "      <td>of</td>\n",
       "      <td>178</td>\n",
       "      <td>in</td>\n",
       "      <td>189</td>\n",
       "      <td>...</td>\n",
       "      <td>in</td>\n",
       "      <td>55</td>\n",
       "      <td>she</td>\n",
       "      <td>35</td>\n",
       "      <td>of</td>\n",
       "      <td>15</td>\n",
       "      <td>in</td>\n",
       "      <td>486</td>\n",
       "      <td>king</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a</td>\n",
       "      <td>84</td>\n",
       "      <td>a</td>\n",
       "      <td>57</td>\n",
       "      <td>in</td>\n",
       "      <td>9</td>\n",
       "      <td>replication</td>\n",
       "      <td>143</td>\n",
       "      <td>and</td>\n",
       "      <td>173</td>\n",
       "      <td>...</td>\n",
       "      <td>of</td>\n",
       "      <td>50</td>\n",
       "      <td>and</td>\n",
       "      <td>30</td>\n",
       "      <td>a</td>\n",
       "      <td>14</td>\n",
       "      <td>and</td>\n",
       "      <td>429</td>\n",
       "      <td>and</td>\n",
       "      <td>403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>to</td>\n",
       "      <td>81</td>\n",
       "      <td>in</td>\n",
       "      <td>56</td>\n",
       "      <td>was</td>\n",
       "      <td>8</td>\n",
       "      <td>a</td>\n",
       "      <td>118</td>\n",
       "      <td>to</td>\n",
       "      <td>136</td>\n",
       "      <td>...</td>\n",
       "      <td>menage</td>\n",
       "      <td>49</td>\n",
       "      <td>of</td>\n",
       "      <td>22</td>\n",
       "      <td>in</td>\n",
       "      <td>11</td>\n",
       "      <td>franklin</td>\n",
       "      <td>402</td>\n",
       "      <td>in</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>is</td>\n",
       "      <td>62</td>\n",
       "      <td>cell</td>\n",
       "      <td>54</td>\n",
       "      <td>and</td>\n",
       "      <td>8</td>\n",
       "      <td>in</td>\n",
       "      <td>102</td>\n",
       "      <td>a</td>\n",
       "      <td>121</td>\n",
       "      <td>...</td>\n",
       "      <td>a</td>\n",
       "      <td>41</td>\n",
       "      <td>team</td>\n",
       "      <td>20</td>\n",
       "      <td>haston</td>\n",
       "      <td>7</td>\n",
       "      <td>a</td>\n",
       "      <td>364</td>\n",
       "      <td>to</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>as</td>\n",
       "      <td>59</td>\n",
       "      <td>is</td>\n",
       "      <td>53</td>\n",
       "      <td>royals</td>\n",
       "      <td>7</td>\n",
       "      <td>and</td>\n",
       "      <td>101</td>\n",
       "      <td>oktoberfest</td>\n",
       "      <td>105</td>\n",
       "      <td>...</td>\n",
       "      <td>to</td>\n",
       "      <td>39</td>\n",
       "      <td>ohea</td>\n",
       "      <td>16</td>\n",
       "      <td>book</td>\n",
       "      <td>5</td>\n",
       "      <td>to</td>\n",
       "      <td>345</td>\n",
       "      <td>a</td>\n",
       "      <td>284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>in</td>\n",
       "      <td>54</td>\n",
       "      <td>as</td>\n",
       "      <td>42</td>\n",
       "      <td>he</td>\n",
       "      <td>6</td>\n",
       "      <td>to</td>\n",
       "      <td>89</td>\n",
       "      <td>is</td>\n",
       "      <td>80</td>\n",
       "      <td>...</td>\n",
       "      <td>s</td>\n",
       "      <td>26</td>\n",
       "      <td>for</td>\n",
       "      <td>16</td>\n",
       "      <td>is</td>\n",
       "      <td>4</td>\n",
       "      <td>his</td>\n",
       "      <td>231</td>\n",
       "      <td>that</td>\n",
       "      <td>209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>that</td>\n",
       "      <td>41</td>\n",
       "      <td>air</td>\n",
       "      <td>41</td>\n",
       "      <td>hall</td>\n",
       "      <td>5</td>\n",
       "      <td>is</td>\n",
       "      <td>83</td>\n",
       "      <td>was</td>\n",
       "      <td>64</td>\n",
       "      <td>...</td>\n",
       "      <td>was</td>\n",
       "      <td>24</td>\n",
       "      <td>was</td>\n",
       "      <td>16</td>\n",
       "      <td>rising</td>\n",
       "      <td>4</td>\n",
       "      <td>he</td>\n",
       "      <td>217</td>\n",
       "      <td>was</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>with</td>\n",
       "      <td>38</td>\n",
       "      <td>to</td>\n",
       "      <td>37</td>\n",
       "      <td>to</td>\n",
       "      <td>5</td>\n",
       "      <td>strand</td>\n",
       "      <td>76</td>\n",
       "      <td>for</td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>he</td>\n",
       "      <td>24</td>\n",
       "      <td>at</td>\n",
       "      <td>15</td>\n",
       "      <td>dead</td>\n",
       "      <td>4</td>\n",
       "      <td>was</td>\n",
       "      <td>210</td>\n",
       "      <td>s</td>\n",
       "      <td>179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>or</td>\n",
       "      <td>32</td>\n",
       "      <td>at</td>\n",
       "      <td>34</td>\n",
       "      <td>season</td>\n",
       "      <td>5</td>\n",
       "      <td>are</td>\n",
       "      <td>40</td>\n",
       "      <td></td>\n",
       "      <td>55</td>\n",
       "      <td>...</td>\n",
       "      <td>his</td>\n",
       "      <td>17</td>\n",
       "      <td>a</td>\n",
       "      <td>15</td>\n",
       "      <td>moon</td>\n",
       "      <td>4</td>\n",
       "      <td>that</td>\n",
       "      <td>142</td>\n",
       "      <td>his</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>are</td>\n",
       "      <td>31</td>\n",
       "      <td>it</td>\n",
       "      <td>33</td>\n",
       "      <td>as</td>\n",
       "      <td>4</td>\n",
       "      <td>by</td>\n",
       "      <td>37</td>\n",
       "      <td>with</td>\n",
       "      <td>51</td>\n",
       "      <td>...</td>\n",
       "      <td>minneapolis</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>15</td>\n",
       "      <td>echo</td>\n",
       "      <td>4</td>\n",
       "      <td>as</td>\n",
       "      <td>136</td>\n",
       "      <td>he</td>\n",
       "      <td>164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>santera</td>\n",
       "      <td>28</td>\n",
       "      <td>circulation</td>\n",
       "      <td>32</td>\n",
       "      <td>basketball</td>\n",
       "      <td>4</td>\n",
       "      <td>polymerase</td>\n",
       "      <td>36</td>\n",
       "      <td>beer</td>\n",
       "      <td>47</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>with</td>\n",
       "      <td>13</td>\n",
       "      <td>magical</td>\n",
       "      <td>4</td>\n",
       "      <td>s</td>\n",
       "      <td>133</td>\n",
       "      <td>for</td>\n",
       "      <td>138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>be</td>\n",
       "      <td>24</td>\n",
       "      <td>are</td>\n",
       "      <td>32</td>\n",
       "      <td>coach</td>\n",
       "      <td>4</td>\n",
       "      <td>this</td>\n",
       "      <td>32</td>\n",
       "      <td>festival</td>\n",
       "      <td>47</td>\n",
       "      <td>...</td>\n",
       "      <td>for</td>\n",
       "      <td>15</td>\n",
       "      <td>to</td>\n",
       "      <td>13</td>\n",
       "      <td>whispers</td>\n",
       "      <td>4</td>\n",
       "      <td>on</td>\n",
       "      <td>133</td>\n",
       "      <td>on</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>for</td>\n",
       "      <td>23</td>\n",
       "      <td>hadley</td>\n",
       "      <td>31</td>\n",
       "      <td>of</td>\n",
       "      <td>4</td>\n",
       "      <td>as</td>\n",
       "      <td>30</td>\n",
       "      <td>at</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>with</td>\n",
       "      <td>14</td>\n",
       "      <td>season</td>\n",
       "      <td>13</td>\n",
       "      <td>published</td>\n",
       "      <td>4</td>\n",
       "      <td>for</td>\n",
       "      <td>130</td>\n",
       "      <td>with</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>their</td>\n",
       "      <td>23</td>\n",
       "      <td>polar</td>\n",
       "      <td>29</td>\n",
       "      <td>with</td>\n",
       "      <td>4</td>\n",
       "      <td>on</td>\n",
       "      <td>30</td>\n",
       "      <td>on</td>\n",
       "      <td>41</td>\n",
       "      <td>...</td>\n",
       "      <td>king</td>\n",
       "      <td>13</td>\n",
       "      <td>australian</td>\n",
       "      <td>12</td>\n",
       "      <td>for</td>\n",
       "      <td>4</td>\n",
       "      <td>by</td>\n",
       "      <td>114</td>\n",
       "      <td>by</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>by</td>\n",
       "      <td>21</td>\n",
       "      <td>by</td>\n",
       "      <td>27</td>\n",
       "      <td>for</td>\n",
       "      <td>4</td>\n",
       "      <td>for</td>\n",
       "      <td>29</td>\n",
       "      <td>by</td>\n",
       "      <td>40</td>\n",
       "      <td>...</td>\n",
       "      <td>by</td>\n",
       "      <td>13</td>\n",
       "      <td>2012</td>\n",
       "      <td>11</td>\n",
       "      <td>elephants</td>\n",
       "      <td>4</td>\n",
       "      <td>with</td>\n",
       "      <td>96</td>\n",
       "      <td>i</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>orichs</td>\n",
       "      <td>21</td>\n",
       "      <td>ferrel</td>\n",
       "      <td>27</td>\n",
       "      <td>also</td>\n",
       "      <td>3</td>\n",
       "      <td>strands</td>\n",
       "      <td>29</td>\n",
       "      <td>s</td>\n",
       "      <td>39</td>\n",
       "      <td>...</td>\n",
       "      <td>land</td>\n",
       "      <td>12</td>\n",
       "      <td>basketball</td>\n",
       "      <td>11</td>\n",
       "      <td>s</td>\n",
       "      <td>4</td>\n",
       "      <td>from</td>\n",
       "      <td>80</td>\n",
       "      <td>rights</td>\n",
       "      <td>83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>known</td>\n",
       "      <td>19</td>\n",
       "      <td>that</td>\n",
       "      <td>25</td>\n",
       "      <td>at</td>\n",
       "      <td>3</td>\n",
       "      <td>from</td>\n",
       "      <td>28</td>\n",
       "      <td>tents</td>\n",
       "      <td>39</td>\n",
       "      <td>...</td>\n",
       "      <td>were</td>\n",
       "      <td>11</td>\n",
       "      <td>national</td>\n",
       "      <td>11</td>\n",
       "      <td>was</td>\n",
       "      <td>4</td>\n",
       "      <td>it</td>\n",
       "      <td>64</td>\n",
       "      <td>as</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>traditional</td>\n",
       "      <td>19</td>\n",
       "      <td>surface</td>\n",
       "      <td>22</td>\n",
       "      <td>seton</td>\n",
       "      <td>3</td>\n",
       "      <td>template</td>\n",
       "      <td>28</td>\n",
       "      <td>tent</td>\n",
       "      <td>39</td>\n",
       "      <td>...</td>\n",
       "      <td>real</td>\n",
       "      <td>10</td>\n",
       "      <td>her</td>\n",
       "      <td>11</td>\n",
       "      <td>to</td>\n",
       "      <td>4</td>\n",
       "      <td>philadelphia</td>\n",
       "      <td>59</td>\n",
       "      <td>civil</td>\n",
       "      <td>76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>from</td>\n",
       "      <td>19</td>\n",
       "      <td>cells</td>\n",
       "      <td>18</td>\n",
       "      <td>after</td>\n",
       "      <td>3</td>\n",
       "      <td>with</td>\n",
       "      <td>28</td>\n",
       "      <td>munich</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>estate</td>\n",
       "      <td>10</td>\n",
       "      <td>game</td>\n",
       "      <td>10</td>\n",
       "      <td>from</td>\n",
       "      <td>3</td>\n",
       "      <td>benjamin</td>\n",
       "      <td>58</td>\n",
       "      <td>at</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>healing</td>\n",
       "      <td>17</td>\n",
       "      <td>this</td>\n",
       "      <td>17</td>\n",
       "      <td>team</td>\n",
       "      <td>3</td>\n",
       "      <td>that</td>\n",
       "      <td>28</td>\n",
       "      <td>as</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>city</td>\n",
       "      <td>10</td>\n",
       "      <td>wnbl</td>\n",
       "      <td>9</td>\n",
       "      <td>fiction</td>\n",
       "      <td>3</td>\n",
       "      <td>an</td>\n",
       "      <td>58</td>\n",
       "      <td>from</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>pacific</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>fork</td>\n",
       "      <td>24</td>\n",
       "      <td>from</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>minnesota</td>\n",
       "      <td>9</td>\n",
       "      <td>played</td>\n",
       "      <td>9</td>\n",
       "      <td>historical</td>\n",
       "      <td>3</td>\n",
       "      <td>which</td>\n",
       "      <td>58</td>\n",
       "      <td>an</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>an</td>\n",
       "      <td>16</td>\n",
       "      <td>from</td>\n",
       "      <td>16</td>\n",
       "      <td>star</td>\n",
       "      <td>3</td>\n",
       "      <td>nucleotides</td>\n",
       "      <td>22</td>\n",
       "      <td>are</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>as</td>\n",
       "      <td>8</td>\n",
       "      <td>s</td>\n",
       "      <td>7</td>\n",
       "      <td>blood</td>\n",
       "      <td>3</td>\n",
       "      <td>had</td>\n",
       "      <td>55</td>\n",
       "      <td>it</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>also</td>\n",
       "      <td>15</td>\n",
       "      <td>on</td>\n",
       "      <td>16</td>\n",
       "      <td>were</td>\n",
       "      <td>3</td>\n",
       "      <td>at</td>\n",
       "      <td>22</td>\n",
       "      <td>this</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>at</td>\n",
       "      <td>8</td>\n",
       "      <td>points</td>\n",
       "      <td>7</td>\n",
       "      <td>doves</td>\n",
       "      <td>3</td>\n",
       "      <td>pennsylvania</td>\n",
       "      <td>54</td>\n",
       "      <td>had</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>they</td>\n",
       "      <td>15</td>\n",
       "      <td>s</td>\n",
       "      <td>16</td>\n",
       "      <td>nba</td>\n",
       "      <td>3</td>\n",
       "      <td>synthesis</td>\n",
       "      <td>22</td>\n",
       "      <td>first</td>\n",
       "      <td>32</td>\n",
       "      <td>...</td>\n",
       "      <td>had</td>\n",
       "      <td>8</td>\n",
       "      <td>per</td>\n",
       "      <td>7</td>\n",
       "      <td>2013</td>\n",
       "      <td>3</td>\n",
       "      <td>is</td>\n",
       "      <td>52</td>\n",
       "      <td>have</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>on</td>\n",
       "      <td>14</td>\n",
       "      <td>which</td>\n",
       "      <td>16</td>\n",
       "      <td>his</td>\n",
       "      <td>3</td>\n",
       "      <td>each</td>\n",
       "      <td>20</td>\n",
       "      <td>were</td>\n",
       "      <td>26</td>\n",
       "      <td>...</td>\n",
       "      <td>new</td>\n",
       "      <td>8</td>\n",
       "      <td>jenna</td>\n",
       "      <td>6</td>\n",
       "      <td>an</td>\n",
       "      <td>3</td>\n",
       "      <td>at</td>\n",
       "      <td>52</td>\n",
       "      <td>is</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>orich</td>\n",
       "      <td>14</td>\n",
       "      <td>equator</td>\n",
       "      <td>14</td>\n",
       "      <td>on</td>\n",
       "      <td>3</td>\n",
       "      <td>process</td>\n",
       "      <td>20</td>\n",
       "      <td>it</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>business</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>career</td>\n",
       "      <td>3</td>\n",
       "      <td>first</td>\n",
       "      <td>51</td>\n",
       "      <td>march</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>which</td>\n",
       "      <td>14</td>\n",
       "      <td>winds</td>\n",
       "      <td>14</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>these</td>\n",
       "      <td>19</td>\n",
       "      <td>which</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>from</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>october</td>\n",
       "      <td>3</td>\n",
       "      <td>american</td>\n",
       "      <td>49</td>\n",
       "      <td>luther</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>religion</td>\n",
       "      <td>12</td>\n",
       "      <td>toward</td>\n",
       "      <td>14</td>\n",
       "      <td>january</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>19</td>\n",
       "      <td>an</td>\n",
       "      <td>23</td>\n",
       "      <td>...</td>\n",
       "      <td>on</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>class</td>\n",
       "      <td>3</td>\n",
       "      <td>be</td>\n",
       "      <td>47</td>\n",
       "      <td>movement</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3732</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>carriers</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3733</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>cv13</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3734</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>bon</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3735</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>homme</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3736</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>cv31</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>operation</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>ii</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3739</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>distinction</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3740</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>operational</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3741</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>1797</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3742</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>franklinia</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3743</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>alatamaha</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3744</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>tree</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3745</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>philadelphians</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3746</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>botanists</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3747</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>bartram</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3748</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>cma</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3749</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>cgm</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3750</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>chinesebuilt</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3751</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>explorerclass</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3752</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>container</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3753</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>193</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3754</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>mayhew</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3755</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>assistant</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3756</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>15th</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3757</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>fled</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3758</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>purified</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3759</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>congregationalist</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3760</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>christianity</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3761</th>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>194</td>\n",
       "      <td>1</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3762 rows  40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             doc1 feq_doc1         doc2 feq_doc2        doc3 feq_doc3  \\\n",
       "0             the      273          the      322         the       25   \n",
       "1              of      138           of      111      wanzer       15   \n",
       "2             and      123          and       91           a        9   \n",
       "3               a       84            a       57          in        9   \n",
       "4              to       81           in       56         was        8   \n",
       "5              is       62         cell       54         and        8   \n",
       "6              as       59           is       53      royals        7   \n",
       "7              in       54           as       42          he        6   \n",
       "8            that       41          air       41        hall        5   \n",
       "9            with       38           to       37          to        5   \n",
       "10             or       32           at       34      season        5   \n",
       "11            are       31           it       33          as        4   \n",
       "12       santera       28  circulation       32  basketball        4   \n",
       "13             be       24          are       32       coach        4   \n",
       "14            for       23       hadley       31          of        4   \n",
       "15          their       23        polar       29        with        4   \n",
       "16             by       21           by       27         for        4   \n",
       "17        orichs       21       ferrel       27        also        3   \n",
       "18          known       19         that       25          at        3   \n",
       "19    traditional       19      surface       22       seton        3   \n",
       "20           from       19        cells       18       after        3   \n",
       "21        healing       17         this       17        team        3   \n",
       "22              6       17      pacific       17           1        3   \n",
       "23             an       16         from       16        star        3   \n",
       "24           also       15           on       16        were        3   \n",
       "25           they       15            s       16         nba        3   \n",
       "26             on       14        which       16         his        3   \n",
       "27         orich       14      equator       14          on        3   \n",
       "28          which       14        winds       14           4        2   \n",
       "29       religion       12       toward       14     january        2   \n",
       "...           ...      ...          ...      ...         ...      ...   \n",
       "3732            -        -            -        -           -        -   \n",
       "3733            -        -            -        -           -        -   \n",
       "3734            -        -            -        -           -        -   \n",
       "3735            -        -            -        -           -        -   \n",
       "3736            -        -            -        -           -        -   \n",
       "3737            -        -            -        -           -        -   \n",
       "3738            -        -            -        -           -        -   \n",
       "3739            -        -            -        -           -        -   \n",
       "3740            -        -            -        -           -        -   \n",
       "3741            -        -            -        -           -        -   \n",
       "3742            -        -            -        -           -        -   \n",
       "3743            -        -            -        -           -        -   \n",
       "3744            -        -            -        -           -        -   \n",
       "3745            -        -            -        -           -        -   \n",
       "3746            -        -            -        -           -        -   \n",
       "3747            -        -            -        -           -        -   \n",
       "3748            -        -            -        -           -        -   \n",
       "3749            -        -            -        -           -        -   \n",
       "3750            -        -            -        -           -        -   \n",
       "3751            -        -            -        -           -        -   \n",
       "3752            -        -            -        -           -        -   \n",
       "3753            -        -            -        -           -        -   \n",
       "3754            -        -            -        -           -        -   \n",
       "3755            -        -            -        -           -        -   \n",
       "3756            -        -            -        -           -        -   \n",
       "3757            -        -            -        -           -        -   \n",
       "3758            -        -            -        -           -        -   \n",
       "3759            -        -            -        -           -        -   \n",
       "3760            -        -            -        -           -        -   \n",
       "3761            -        -            -        -           -        -   \n",
       "\n",
       "             doc4 feq_doc4         doc5 feq_doc5  ...        doc16 feq_doc16  \\\n",
       "0             the      360          the      612  ...          the       106   \n",
       "1             dna      200           of      191  ...          and        56   \n",
       "2              of      178           in      189  ...           in        55   \n",
       "3     replication      143          and      173  ...           of        50   \n",
       "4               a      118           to      136  ...       menage        49   \n",
       "5              in      102            a      121  ...            a        41   \n",
       "6             and      101  oktoberfest      105  ...           to        39   \n",
       "7              to       89           is       80  ...            s        26   \n",
       "8              is       83          was       64  ...          was        24   \n",
       "9          strand       76          for       55  ...           he        24   \n",
       "10            are       40                   55  ...          his        17   \n",
       "11             by       37         with       51  ...  minneapolis        15   \n",
       "12     polymerase       36         beer       47  ...            1        15   \n",
       "13           this       32     festival       47  ...          for        15   \n",
       "14             as       30           at       43  ...         with        14   \n",
       "15             on       30           on       41  ...         king        13   \n",
       "16            for       29           by       40  ...           by        13   \n",
       "17        strands       29            s       39  ...         land        12   \n",
       "18           from       28        tents       39  ...         were        11   \n",
       "19       template       28         tent       39  ...         real        10   \n",
       "20           with       28       munich       37  ...       estate        10   \n",
       "21           that       28           as       37  ...         city        10   \n",
       "22           fork       24         from       36  ...    minnesota         9   \n",
       "23    nucleotides       22          are       36  ...           as         8   \n",
       "24             at       22         this       36  ...           at         8   \n",
       "25      synthesis       22        first       32  ...          had         8   \n",
       "26           each       20         were       26  ...          new         8   \n",
       "27        process       20           it       24  ...     business         8   \n",
       "28          these       19        which       24  ...         from         7   \n",
       "29              3       19           an       23  ...           on         7   \n",
       "...           ...      ...          ...      ...  ...          ...       ...   \n",
       "3732            -        -            -        -  ...            -         -   \n",
       "3733            -        -            -        -  ...            -         -   \n",
       "3734            -        -            -        -  ...            -         -   \n",
       "3735            -        -            -        -  ...            -         -   \n",
       "3736            -        -            -        -  ...            -         -   \n",
       "3737            -        -            -        -  ...            -         -   \n",
       "3738            -        -            -        -  ...            -         -   \n",
       "3739            -        -            -        -  ...            -         -   \n",
       "3740            -        -            -        -  ...            -         -   \n",
       "3741            -        -            -        -  ...            -         -   \n",
       "3742            -        -            -        -  ...            -         -   \n",
       "3743            -        -            -        -  ...            -         -   \n",
       "3744            -        -            -        -  ...            -         -   \n",
       "3745            -        -            -        -  ...            -         -   \n",
       "3746            -        -            -        -  ...            -         -   \n",
       "3747            -        -            -        -  ...            -         -   \n",
       "3748            -        -            -        -  ...            -         -   \n",
       "3749            -        -            -        -  ...            -         -   \n",
       "3750            -        -            -        -  ...            -         -   \n",
       "3751            -        -            -        -  ...            -         -   \n",
       "3752            -        -            -        -  ...            -         -   \n",
       "3753            -        -            -        -  ...            -         -   \n",
       "3754            -        -            -        -  ...            -         -   \n",
       "3755            -        -            -        -  ...            -         -   \n",
       "3756            -        -            -        -  ...            -         -   \n",
       "3757            -        -            -        -  ...            -         -   \n",
       "3758            -        -            -        -  ...            -         -   \n",
       "3759            -        -            -        -  ...            -         -   \n",
       "3760            -        -            -        -  ...            -         -   \n",
       "3761            -        -            -        -  ...            -         -   \n",
       "\n",
       "           doc17 feq_doc17       doc18 feq_doc18              doc19 feq_doc19  \\\n",
       "0            the        80         the        18                the       894   \n",
       "1             in        57         and        16                 of       600   \n",
       "2            she        35          of        15                 in       486   \n",
       "3            and        30           a        14                and       429   \n",
       "4             of        22          in        11           franklin       402   \n",
       "5           team        20      haston         7                  a       364   \n",
       "6           ohea        16        book         5                 to       345   \n",
       "7            for        16          is         4                his       231   \n",
       "8            was        16      rising         4                 he       217   \n",
       "9             at        15        dead         4                was       210   \n",
       "10             a        15        moon         4               that       142   \n",
       "11             3        15        echo         4                 as       136   \n",
       "12          with        13     magical         4                  s       133   \n",
       "13            to        13    whispers         4                 on       133   \n",
       "14        season        13   published         4                for       130   \n",
       "15    australian        12         for         4                 by       114   \n",
       "16          2012        11   elephants         4               with        96   \n",
       "17    basketball        11           s         4               from        80   \n",
       "18      national        11         was         4                 it        64   \n",
       "19           her        11          to         4       philadelphia        59   \n",
       "20          game        10        from         3           benjamin        58   \n",
       "21          wnbl         9     fiction         3                 an        58   \n",
       "22        played         9  historical         3              which        58   \n",
       "23             s         7       blood         3                had        55   \n",
       "24        points         7       doves         3       pennsylvania        54   \n",
       "25           per         7        2013         3                 is        52   \n",
       "26         jenna         6          an         3                 at        52   \n",
       "27             2         6      career         3              first        51   \n",
       "28             4         6     october         3           american        49   \n",
       "29             6         6       class         3                 be        47   \n",
       "...          ...       ...         ...       ...                ...       ...   \n",
       "3732           -         -           -         -           carriers         1   \n",
       "3733           -         -           -         -               cv13         1   \n",
       "3734           -         -           -         -                bon         1   \n",
       "3735           -         -           -         -              homme         1   \n",
       "3736           -         -           -         -               cv31         1   \n",
       "3737           -         -           -         -          operation         1   \n",
       "3738           -         -           -         -                 ii         1   \n",
       "3739           -         -           -         -        distinction         1   \n",
       "3740           -         -           -         -        operational         1   \n",
       "3741           -         -           -         -               1797         1   \n",
       "3742           -         -           -         -         franklinia         1   \n",
       "3743           -         -           -         -          alatamaha         1   \n",
       "3744           -         -           -         -               tree         1   \n",
       "3745           -         -           -         -     philadelphians         1   \n",
       "3746           -         -           -         -          botanists         1   \n",
       "3747           -         -           -         -            bartram         1   \n",
       "3748           -         -           -         -                cma         1   \n",
       "3749           -         -           -         -                cgm         1   \n",
       "3750           -         -           -         -       chinesebuilt         1   \n",
       "3751           -         -           -         -      explorerclass         1   \n",
       "3752           -         -           -         -          container         1   \n",
       "3753           -         -           -         -                193         1   \n",
       "3754           -         -           -         -             mayhew         1   \n",
       "3755           -         -           -         -          assistant         1   \n",
       "3756           -         -           -         -               15th         1   \n",
       "3757           -         -           -         -               fled         1   \n",
       "3758           -         -           -         -           purified         1   \n",
       "3759           -         -           -         -  congregationalist         1   \n",
       "3760           -         -           -         -       christianity         1   \n",
       "3761           -         -           -         -                194         1   \n",
       "\n",
       "         doc20 feq_doc20  \n",
       "0          the       963  \n",
       "1           of       477  \n",
       "2         king       414  \n",
       "3          and       403  \n",
       "4           in       387  \n",
       "5           to       387  \n",
       "6            a       284  \n",
       "7         that       209  \n",
       "8          was       198  \n",
       "9            s       179  \n",
       "10         his       169  \n",
       "11          he       164  \n",
       "12         for       138  \n",
       "13          on       127  \n",
       "14        with       108  \n",
       "15          by        95  \n",
       "16           i        95  \n",
       "17      rights        83  \n",
       "18          as        82  \n",
       "19       civil        76  \n",
       "20          at        71  \n",
       "21        from        66  \n",
       "22          an        65  \n",
       "23          it        60  \n",
       "24         had        59  \n",
       "25        have        55  \n",
       "26          is        54  \n",
       "27       march        53  \n",
       "28      luther        52  \n",
       "29    movement        52  \n",
       "...        ...       ...  \n",
       "3732         -         -  \n",
       "3733         -         -  \n",
       "3734         -         -  \n",
       "3735         -         -  \n",
       "3736         -         -  \n",
       "3737         -         -  \n",
       "3738         -         -  \n",
       "3739         -         -  \n",
       "3740         -         -  \n",
       "3741         -         -  \n",
       "3742         -         -  \n",
       "3743         -         -  \n",
       "3744         -         -  \n",
       "3745         -         -  \n",
       "3746         -         -  \n",
       "3747         -         -  \n",
       "3748         -         -  \n",
       "3749         -         -  \n",
       "3750         -         -  \n",
       "3751         -         -  \n",
       "3752         -         -  \n",
       "3753         -         -  \n",
       "3754         -         -  \n",
       "3755         -         -  \n",
       "3756         -         -  \n",
       "3757         -         -  \n",
       "3758         -         -  \n",
       "3759         -         -  \n",
       "3760         -         -  \n",
       "3761         -         -  \n",
       "\n",
       "[3762 rows x 40 columns]"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=pd.concat([doc1,doc2,doc3,doc4,doc5,doc6,doc7,doc8,doc9,doc10\n",
    "              ,doc11,doc12,doc13,doc14,doc15,doc16,doc17,doc18\n",
    "              ,doc19,doc20], axis=1, sort=False).fillna('-')\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  add row that give total number of words in each documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.loc['sum'] = doc.sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save file to csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc.to_csv(r'result2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>doc1</th>\n",
       "      <th>feq_doc1</th>\n",
       "      <th>doc2</th>\n",
       "      <th>feq_doc2</th>\n",
       "      <th>doc3</th>\n",
       "      <th>feq_doc3</th>\n",
       "      <th>doc4</th>\n",
       "      <th>feq_doc4</th>\n",
       "      <th>doc5</th>\n",
       "      <th>...</th>\n",
       "      <th>doc16</th>\n",
       "      <th>feq_doc16</th>\n",
       "      <th>doc17</th>\n",
       "      <th>feq_doc17</th>\n",
       "      <th>doc18</th>\n",
       "      <th>feq_doc18</th>\n",
       "      <th>doc19</th>\n",
       "      <th>feq_doc19</th>\n",
       "      <th>doc20</th>\n",
       "      <th>feq_doc20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>the</td>\n",
       "      <td>273.0</td>\n",
       "      <td>the</td>\n",
       "      <td>322.0</td>\n",
       "      <td>the</td>\n",
       "      <td>25.0</td>\n",
       "      <td>the</td>\n",
       "      <td>360.0</td>\n",
       "      <td>the</td>\n",
       "      <td>...</td>\n",
       "      <td>the</td>\n",
       "      <td>106.0</td>\n",
       "      <td>the</td>\n",
       "      <td>80.0</td>\n",
       "      <td>the</td>\n",
       "      <td>18.0</td>\n",
       "      <td>the</td>\n",
       "      <td>894</td>\n",
       "      <td>the</td>\n",
       "      <td>963.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>of</td>\n",
       "      <td>138.0</td>\n",
       "      <td>of</td>\n",
       "      <td>111.0</td>\n",
       "      <td>wanzer</td>\n",
       "      <td>15.0</td>\n",
       "      <td>dna</td>\n",
       "      <td>200.0</td>\n",
       "      <td>of</td>\n",
       "      <td>...</td>\n",
       "      <td>and</td>\n",
       "      <td>56.0</td>\n",
       "      <td>in</td>\n",
       "      <td>57.0</td>\n",
       "      <td>and</td>\n",
       "      <td>16.0</td>\n",
       "      <td>of</td>\n",
       "      <td>600</td>\n",
       "      <td>of</td>\n",
       "      <td>477.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>and</td>\n",
       "      <td>123.0</td>\n",
       "      <td>and</td>\n",
       "      <td>91.0</td>\n",
       "      <td>a</td>\n",
       "      <td>9.0</td>\n",
       "      <td>of</td>\n",
       "      <td>178.0</td>\n",
       "      <td>in</td>\n",
       "      <td>...</td>\n",
       "      <td>in</td>\n",
       "      <td>55.0</td>\n",
       "      <td>she</td>\n",
       "      <td>35.0</td>\n",
       "      <td>of</td>\n",
       "      <td>15.0</td>\n",
       "      <td>in</td>\n",
       "      <td>486</td>\n",
       "      <td>king</td>\n",
       "      <td>414.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "      <td>84.0</td>\n",
       "      <td>a</td>\n",
       "      <td>57.0</td>\n",
       "      <td>in</td>\n",
       "      <td>9.0</td>\n",
       "      <td>replication</td>\n",
       "      <td>143.0</td>\n",
       "      <td>and</td>\n",
       "      <td>...</td>\n",
       "      <td>of</td>\n",
       "      <td>50.0</td>\n",
       "      <td>and</td>\n",
       "      <td>30.0</td>\n",
       "      <td>a</td>\n",
       "      <td>14.0</td>\n",
       "      <td>and</td>\n",
       "      <td>429</td>\n",
       "      <td>and</td>\n",
       "      <td>403.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>to</td>\n",
       "      <td>81.0</td>\n",
       "      <td>in</td>\n",
       "      <td>56.0</td>\n",
       "      <td>was</td>\n",
       "      <td>8.0</td>\n",
       "      <td>a</td>\n",
       "      <td>118.0</td>\n",
       "      <td>to</td>\n",
       "      <td>...</td>\n",
       "      <td>menage</td>\n",
       "      <td>49.0</td>\n",
       "      <td>of</td>\n",
       "      <td>22.0</td>\n",
       "      <td>in</td>\n",
       "      <td>11.0</td>\n",
       "      <td>franklin</td>\n",
       "      <td>402</td>\n",
       "      <td>in</td>\n",
       "      <td>387.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Unnamed: 0 doc1 feq_doc1 doc2 feq_doc2    doc3 feq_doc3         doc4  \\\n",
       "0          0  the    273.0  the    322.0     the     25.0          the   \n",
       "1          1   of    138.0   of    111.0  wanzer     15.0          dna   \n",
       "2          2  and    123.0  and     91.0       a      9.0           of   \n",
       "3          3    a     84.0    a     57.0      in      9.0  replication   \n",
       "4          4   to     81.0   in     56.0     was      8.0            a   \n",
       "\n",
       "  feq_doc4 doc5  ...   doc16 feq_doc16 doc17 feq_doc17 doc18 feq_doc18  \\\n",
       "0    360.0  the  ...     the     106.0   the      80.0   the      18.0   \n",
       "1    200.0   of  ...     and      56.0    in      57.0   and      16.0   \n",
       "2    178.0   in  ...      in      55.0   she      35.0    of      15.0   \n",
       "3    143.0  and  ...      of      50.0   and      30.0     a      14.0   \n",
       "4    118.0   to  ...  menage      49.0    of      22.0    in      11.0   \n",
       "\n",
       "      doc19 feq_doc19 doc20 feq_doc20  \n",
       "0       the       894   the     963.0  \n",
       "1        of       600    of     477.0  \n",
       "2        in       486  king     414.0  \n",
       "3       and       429   and     403.0  \n",
       "4  franklin       402    in     387.0  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2= pd.read_csv('result2.csv')\n",
    "result2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word search in all documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>doc1</th>\n",
       "      <th>feq_doc1</th>\n",
       "      <th>doc2</th>\n",
       "      <th>feq_doc2</th>\n",
       "      <th>doc3</th>\n",
       "      <th>feq_doc3</th>\n",
       "      <th>doc4</th>\n",
       "      <th>feq_doc4</th>\n",
       "      <th>doc5</th>\n",
       "      <th>...</th>\n",
       "      <th>doc16</th>\n",
       "      <th>feq_doc16</th>\n",
       "      <th>doc17</th>\n",
       "      <th>feq_doc17</th>\n",
       "      <th>doc18</th>\n",
       "      <th>feq_doc18</th>\n",
       "      <th>doc19</th>\n",
       "      <th>feq_doc19</th>\n",
       "      <th>doc20</th>\n",
       "      <th>feq_doc20</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>the</td>\n",
       "      <td>273.0</td>\n",
       "      <td>the</td>\n",
       "      <td>322.0</td>\n",
       "      <td>the</td>\n",
       "      <td>25.0</td>\n",
       "      <td>the</td>\n",
       "      <td>360.0</td>\n",
       "      <td>the</td>\n",
       "      <td>...</td>\n",
       "      <td>the</td>\n",
       "      <td>106.0</td>\n",
       "      <td>the</td>\n",
       "      <td>80.0</td>\n",
       "      <td>the</td>\n",
       "      <td>18.0</td>\n",
       "      <td>the</td>\n",
       "      <td>894</td>\n",
       "      <td>the</td>\n",
       "      <td>963.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>their</td>\n",
       "      <td>23.0</td>\n",
       "      <td>polar</td>\n",
       "      <td>29.0</td>\n",
       "      <td>with</td>\n",
       "      <td>4.0</td>\n",
       "      <td>on</td>\n",
       "      <td>30.0</td>\n",
       "      <td>on</td>\n",
       "      <td>...</td>\n",
       "      <td>king</td>\n",
       "      <td>13.0</td>\n",
       "      <td>australian</td>\n",
       "      <td>12.0</td>\n",
       "      <td>for</td>\n",
       "      <td>4.0</td>\n",
       "      <td>by</td>\n",
       "      <td>114</td>\n",
       "      <td>by</td>\n",
       "      <td>95.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>they</td>\n",
       "      <td>15.0</td>\n",
       "      <td>s</td>\n",
       "      <td>16.0</td>\n",
       "      <td>nba</td>\n",
       "      <td>3.0</td>\n",
       "      <td>synthesis</td>\n",
       "      <td>22.0</td>\n",
       "      <td>first</td>\n",
       "      <td>...</td>\n",
       "      <td>had</td>\n",
       "      <td>8.0</td>\n",
       "      <td>per</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>3.0</td>\n",
       "      <td>is</td>\n",
       "      <td>52</td>\n",
       "      <td>have</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>60</td>\n",
       "      <td>these</td>\n",
       "      <td>8.0</td>\n",
       "      <td>latitudes</td>\n",
       "      <td>7.0</td>\n",
       "      <td>hooks</td>\n",
       "      <td>1.0</td>\n",
       "      <td>growing</td>\n",
       "      <td>10.0</td>\n",
       "      <td>place</td>\n",
       "      <td>...</td>\n",
       "      <td>time</td>\n",
       "      <td>4.0</td>\n",
       "      <td>injury</td>\n",
       "      <td>4.0</td>\n",
       "      <td>on</td>\n",
       "      <td>2.0</td>\n",
       "      <td>us</td>\n",
       "      <td>23</td>\n",
       "      <td>war</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>71</td>\n",
       "      <td>there</td>\n",
       "      <td>7.0</td>\n",
       "      <td>article</td>\n",
       "      <td>6.0</td>\n",
       "      <td>record</td>\n",
       "      <td>1.0</td>\n",
       "      <td>bases</td>\n",
       "      <td>9.0</td>\n",
       "      <td>paulaner</td>\n",
       "      <td>...</td>\n",
       "      <td>first</td>\n",
       "      <td>4.0</td>\n",
       "      <td>bulleen</td>\n",
       "      <td>3.0</td>\n",
       "      <td>canada</td>\n",
       "      <td>1.0</td>\n",
       "      <td>also</td>\n",
       "      <td>19</td>\n",
       "      <td>all</td>\n",
       "      <td>23.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>251</th>\n",
       "      <td>251</td>\n",
       "      <td>them</td>\n",
       "      <td>2.0</td>\n",
       "      <td>travels</td>\n",
       "      <td>2.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>plasmids</td>\n",
       "      <td>3.0</td>\n",
       "      <td>service</td>\n",
       "      <td>...</td>\n",
       "      <td>again</td>\n",
       "      <td>1.0</td>\n",
       "      <td>metro</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>george</td>\n",
       "      <td>8</td>\n",
       "      <td>hoover</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>656</th>\n",
       "      <td>656</td>\n",
       "      <td>then</td>\n",
       "      <td>1.0</td>\n",
       "      <td>alone</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>includes</td>\n",
       "      <td>1.0</td>\n",
       "      <td>reduce</td>\n",
       "      <td>...</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>-</td>\n",
       "      <td>set</td>\n",
       "      <td>3</td>\n",
       "      <td>based</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3762</th>\n",
       "      <td>sum</td>\n",
       "      <td>theofandatoisasinthatwithoraresanterabeforthe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>theofandaincellisasairtoatitcirculationarehadl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thewanzerainwasandroyalshehalltoseasonasbasket...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thednaofreplicationainandtoisstrandarebypolyme...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>theofinandtoaoktoberfestiswasforwithbeerfesti...</td>\n",
       "      <td>...</td>\n",
       "      <td>theandinofmenageatoswashehisminneapolis1forwit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>theinsheandofteamoheaforwasata3withtoseasonaus...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>theandofainhastonbookisrisingdeadmoonechomagic...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>theofinandfranklinatohishewasthatassonforbywit...</td>\n",
       "      <td>15361</td>\n",
       "      <td>theofkingandintoathatwasshisheforonwithbyirigh...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                                               doc1 feq_doc1  \\\n",
       "0             0                                                the    273.0   \n",
       "15           15                                              their     23.0   \n",
       "25           25                                               they     15.0   \n",
       "60           60                                              these      8.0   \n",
       "71           71                                              there      7.0   \n",
       "251         251                                               them      2.0   \n",
       "656         656                                               then      1.0   \n",
       "3762        sum  theofandatoisasinthatwithoraresanterabeforthe...      NaN   \n",
       "\n",
       "                                                   doc2 feq_doc2  \\\n",
       "0                                                   the    322.0   \n",
       "15                                                polar     29.0   \n",
       "25                                                    s     16.0   \n",
       "60                                            latitudes      7.0   \n",
       "71                                              article      6.0   \n",
       "251                                             travels      2.0   \n",
       "656                                               alone      1.0   \n",
       "3762  theofandaincellisasairtoatitcirculationarehadl...      NaN   \n",
       "\n",
       "                                                   doc3 feq_doc3  \\\n",
       "0                                                   the     25.0   \n",
       "15                                                 with      4.0   \n",
       "25                                                  nba      3.0   \n",
       "60                                                hooks      1.0   \n",
       "71                                               record      1.0   \n",
       "251                                                   -        -   \n",
       "656                                                   -        -   \n",
       "3762  thewanzerainwasandroyalshehalltoseasonasbasket...      NaN   \n",
       "\n",
       "                                                   doc4 feq_doc4  \\\n",
       "0                                                   the    360.0   \n",
       "15                                                   on     30.0   \n",
       "25                                            synthesis     22.0   \n",
       "60                                              growing     10.0   \n",
       "71                                                bases      9.0   \n",
       "251                                            plasmids      3.0   \n",
       "656                                            includes      1.0   \n",
       "3762  thednaofreplicationainandtoisstrandarebypolyme...      NaN   \n",
       "\n",
       "                                                   doc5  ...  \\\n",
       "0                                                   the  ...   \n",
       "15                                                   on  ...   \n",
       "25                                                first  ...   \n",
       "60                                                place  ...   \n",
       "71                                             paulaner  ...   \n",
       "251                                             service  ...   \n",
       "656                                              reduce  ...   \n",
       "3762  theofinandtoaoktoberfestiswasforwithbeerfesti...  ...   \n",
       "\n",
       "                                                  doc16 feq_doc16  \\\n",
       "0                                                   the     106.0   \n",
       "15                                                 king      13.0   \n",
       "25                                                  had       8.0   \n",
       "60                                                 time       4.0   \n",
       "71                                                first       4.0   \n",
       "251                                               again       1.0   \n",
       "656                                                   -         -   \n",
       "3762  theandinofmenageatoswashehisminneapolis1forwit...       NaN   \n",
       "\n",
       "                                                  doc17 feq_doc17  \\\n",
       "0                                                   the      80.0   \n",
       "15                                           australian      12.0   \n",
       "25                                                  per       7.0   \n",
       "60                                               injury       4.0   \n",
       "71                                              bulleen       3.0   \n",
       "251                                               metro       1.0   \n",
       "656                                                   -         -   \n",
       "3762  theinsheandofteamoheaforwasata3withtoseasonaus...       NaN   \n",
       "\n",
       "                                                  doc18 feq_doc18  \\\n",
       "0                                                   the      18.0   \n",
       "15                                                  for       4.0   \n",
       "25                                                 2013       3.0   \n",
       "60                                                   on       2.0   \n",
       "71                                               canada       1.0   \n",
       "251                                                   -         -   \n",
       "656                                                   -         -   \n",
       "3762  theandofainhastonbookisrisingdeadmoonechomagic...       NaN   \n",
       "\n",
       "                                                  doc19 feq_doc19  \\\n",
       "0                                                   the       894   \n",
       "15                                                   by       114   \n",
       "25                                                   is        52   \n",
       "60                                                   us        23   \n",
       "71                                                 also        19   \n",
       "251                                              george         8   \n",
       "656                                                 set         3   \n",
       "3762  theofinandfranklinatohishewasthatassonforbywit...     15361   \n",
       "\n",
       "                                                  doc20 feq_doc20  \n",
       "0                                                   the     963.0  \n",
       "15                                                   by      95.0  \n",
       "25                                                 have      55.0  \n",
       "60                                                  war      24.0  \n",
       "71                                                  all      23.0  \n",
       "251                                              hoover       8.0  \n",
       "656                                               based       3.0  \n",
       "3762  theofkingandintoathatwasshisheforonwithbyirigh...       NaN  \n",
       "\n",
       "[8 rows x 41 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result2[result2['doc1'].str.match('the')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'words_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-4223f83dc74a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mcount_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'the'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwords_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'words_count' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
